{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvStack(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the conv_stack module as described in figure 6\n",
    "    \"\"\"\n",
    "    def __init__(self, k1, c1, k2, c2, k3, c3):\n",
    "        super(ConvStack, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=c1, kernel_size=k1, padding=(k1-1)//2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=c1, out_channels=c2, kernel_size=k2, padding=(k2-1)//2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=c2, out_channels=c3, kernel_size=k3, padding=(k3-1)//2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if not torch.is_tensor(X):\n",
    "            X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "            \n",
    "        conv1_X = self.conv1(X)\n",
    "        conv1_relu = F.relu(conv1_X)\n",
    "        \n",
    "        conv2_X = self.conv2(conv1_X)\n",
    "        conv2_relu = F.relu(conv2_X + conv1_relu)\n",
    "        \n",
    "        conv3_X = self.conv3(conv2_relu)\n",
    "        \n",
    "        return conv3_X\n",
    "    \n",
    "    \n",
    "class ResConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the res_conv module as described in figure 7 in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, use_extra_convolution=True):\n",
    "        super(ResConv, self).__init__()\n",
    "        self.use_extra_convolution = use_extra_convolution\n",
    "        self.extra_conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=1)\n",
    "        \n",
    "        if use_extra_convolution:\n",
    "            conv1_in_channels = 64\n",
    "        else:\n",
    "            conv1_in_channels = 3\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=conv1_in_channels, out_channels=32, kernel_size=3, padding=(3-1)//2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=(5-1)//2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=(3-1)//2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if not torch.is_tensor(X):\n",
    "            X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "        if self.use_extra_convolution:\n",
    "            c = self.extra_conv(X)\n",
    "        else:\n",
    "            c = X\n",
    "        \n",
    "        conv1_relu = F.relu(self.conv1(c))\n",
    "        conv2_relu = F.relu(self.conv2(conv1_relu))\n",
    "        rc3 = self.conv3(conv2_relu)\n",
    "        rc = c + rc3\n",
    "        return rc\n",
    "    \n",
    "\n",
    "class StateTransitionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the state transition function g(s,z,a) as described in figure 9 in the paper\n",
    "    An action, a state and a latent variable at time t-1 is transitioned to \n",
    "    a state at time t.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(StateTransitionModule, self).__init__()\n",
    "        self.res_conv1 = ResConv()\n",
    "        self.res_conv2 = ResConv()\n",
    "        \n",
    "    def pool_inject(self, X):\n",
    "        \"\"\"\n",
    "        Implements the pool & inject module as described in figure 8 in the paper\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(X):\n",
    "            X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "\n",
    "        height, width = X.shape[2:]\n",
    "        m = F.max_pool2d(X, kernel_size=(height, width), stride=(1, 1))\n",
    "        tiled = m.repeat(1, 1, height, width)\n",
    "        pi = torch.cat([tiled, X], axis=1) # concat on the Color channel\n",
    "        return pi\n",
    "    \n",
    "    def forward(self, a, s, z):\n",
    "        if z is None:\n",
    "            c = torch.cat([a, s], axis=1) # concat on the Color channel\n",
    "        else:\n",
    "            c = torch.cat([a, s, z], axis=1) # concat on the Color channel\n",
    "            \n",
    "        rc1_relu = F.relu(self.res_conv1(c))\n",
    "        pi = self.pool_inject(rc1_relu)\n",
    "        s_next = self.res_conv2(pi)\n",
    "        \n",
    "        return s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 96, 96]) torch.Size([1, 3, 96, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 96, 96])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.flip(np.transpose(state, axes=[2, 0, 1])).copy()\n",
    "res_conv = ResConv() \n",
    "pool_inject(np.array([s])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
