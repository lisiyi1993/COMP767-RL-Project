{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from utils.minipacman import MiniPacman\n",
    "from utils.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):  \n",
    "    def __init__(self, in_shape, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.critic_network = nn.Linear(256, 1)\n",
    "        self.actor_network = nn.Linear(256, n_actions)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        logit = self.actor_network(x)\n",
    "        value = self.critic_network(x)\n",
    "        return logit, value\n",
    "    \n",
    "        \n",
    "    def act(self, x, deterministic=False):\n",
    "        logit, _ = self.forward(x)\n",
    "        probs = F.softmax(logit, dim=-1)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = probs.max(1)[1]\n",
    "        else:\n",
    "            action = probs.multinomial(1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def evaluate_actions(self, x, action):\n",
    "        logit, value = self.forward(x)\n",
    "        \n",
    "        probs     = F.softmax(logit, dim=-1)\n",
    "        log_probs = F.log_softmax(logit, dim=-1)\n",
    "        \n",
    "        action_log_probs = log_probs.gather(1, action)\n",
    "        entropy = -(probs * log_probs).sum(1).mean()\n",
    "        \n",
    "        return logit, action_log_probs, value, entropy\n",
    "        \n",
    "    def feature_size(self):\n",
    "        return self.features(torch.zeros(1, *self.in_shape)).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_envs, state_shape):\n",
    "        self.num_steps = num_steps\n",
    "        self.num_envs = num_envs\n",
    "        self.states = torch.zeros(num_steps+1, num_envs, *state_shape)\n",
    "        self.rewards = torch.zeros(num_steps, num_envs, 1)\n",
    "        self.masks = torch.ones(num_steps+1, num_envs, 1)\n",
    "        self.actions = torch.zeros(num_steps, num_envs, 1).long()\n",
    "            \n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        self.states = self.states.to(device)\n",
    "        self.rewards = self.rewards.to(device)\n",
    "        self.masks = self.masks.to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        \n",
    "    def insert(self, step, state, action, reward, mask):\n",
    "        self.states[step+1].copy_(state)\n",
    "        self.actions[step].copy_(action)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.masks[step+1].copy_(mask)\n",
    "        \n",
    "    def after_update(self):\n",
    "        self.states[0].copy_(self.states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        \n",
    "    def compute_returns(self, next_value, gamma):\n",
    "        returns = torch.zeros(self.num_steps + 1, self.num_envs, 1).to(self.device)\n",
    "        returns[-1] = next_value\n",
    "        for step in reversed(range(self.num_steps)):\n",
    "            returns[step] = returns[step+1] * gamma * self.masks[step + 1] + self.rewards[step]\n",
    "        return returns[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"regular\"\n",
    "num_envs = 16\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = MiniPacman(mode, 1000)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "state_shape = envs.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a2c hyperparams:\n",
    "gamma = 0.99\n",
    "entropy_coef = 0.01\n",
    "value_loss_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "num_steps = 10\n",
    "num_frames = int(1e5)\n",
    "\n",
    "#rmsprop hyperparams:\n",
    "lr    = 7e-4\n",
    "eps   = 1e-5\n",
    "alpha = 0.99\n",
    "\n",
    "#Init a2c and rmsprop\n",
    "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
    "actor_critic.to(DEVICE)\n",
    "optimizer = optim.RMSprop(actor_critic.parameters(), lr, eps=eps, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = RolloutStorage(num_steps, num_envs, envs.observation_space.shape)\n",
    "rollout.to(DEVICE)\n",
    "\n",
    "all_rewards = []\n",
    "all_losses  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = envs.reset()\n",
    "state = torch.FloatTensor(np.float32(state)).to(DEVICE)\n",
    "\n",
    "    \n",
    "rollout.states[0].copy_(state)\n",
    "\n",
    "episode_rewards = torch.zeros(num_envs, 1)\n",
    "final_rewards   = torch.zeros(num_envs, 1)\n",
    "\n",
    "for i_update in range(num_frames):\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        action = actor_critic.act(state)\n",
    "        next_state, reward, done, _ = envs.step(action.squeeze(1).cpu().data.numpy())\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "        episode_rewards += reward\n",
    "        masks = torch.FloatTensor(1-np.array(done)).unsqueeze(1)\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1-masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "        \n",
    "        masks.to(DEVICE)\n",
    "\n",
    "        state = torch.FloatTensor(np.float32(next_state)).to(DEVICE)\n",
    "        rollout.insert(step, state, action.data, reward, masks)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, next_value = actor_critic(rollout.states[-1])\n",
    "        \n",
    "    next_value = next_value.data\n",
    "\n",
    "    returns = rollout.compute_returns(next_value, gamma)\n",
    "\n",
    "    logit, action_log_probs, values, entropy = actor_critic.evaluate_actions(\n",
    "        rollout.states[:-1].detach().view(-1, *state_shape),\n",
    "        rollout.actions.detach().view(-1, 1)\n",
    "    )\n",
    "\n",
    "    values = values.view(num_steps, num_envs, 1)\n",
    "    action_log_probs = action_log_probs.view(num_steps, num_envs, 1)\n",
    "    advantages = returns.detach() - values\n",
    "\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "    action_loss = -(advantages.detach() * action_log_probs).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(actor_critic.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i_update % 100 == 0:\n",
    "        all_rewards.append(final_rewards.mean())\n",
    "        all_losses.append(loss.data)\n",
    "        \n",
    "        display.clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title(f\"epoch {i_update}. reward: {np.mean(all_rewards[-10:])}\")\n",
    "        plt.plot(all_rewards)\n",
    "        plt.subplot(132)\n",
    "        plt.title(f\"loss {all_losses[-1].item()}\")\n",
    "        plt.plot(all_losses)\n",
    "        plt.show()\n",
    "        \n",
    "    rollout.after_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor_critic.state_dict(), \"actor_critic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading successfully!\n"
     ]
    }
   ],
   "source": [
    "mode = \"regular\"\n",
    "env = MiniPacman(mode, 1000)\n",
    "\n",
    "actor_critic = ActorCritic(env.observation_space.shape, env.action_space.n)\n",
    "checkpoint = torch.load(os.path.join(\"training\", \"actor_critic\", \"actor_critic_checkpoint\"), map_location=DEVICE)\n",
    "actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])\n",
    "actor_critic.to(DEVICE)\n",
    "\n",
    "print(\"loading successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = MiniPacman(mode, 1000)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "step = 1\n",
    "\n",
    "while not done:\n",
    "    current_state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "    action = actor_critic.act(current_state)\n",
    "    next_state, reward, done, _ = env.step(action.data[0, 0])\n",
    "    total_reward += reward\n",
    "    \n",
    "    plt.imshow(state.transpose([1, 2, 0]))\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"steps: {step}, reward: {total_reward}\")\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    step += 1\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
