{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from utils.minipacman import MiniPacman\n",
    "from utils.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):  \n",
    "    def __init__(self, in_shape, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.critic_network = nn.Linear(256, 1)\n",
    "        self.actor_network = nn.Linear(256, n_actions)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        logit = self.actor_network(x)\n",
    "        value = self.critic_network(x)\n",
    "        return logit, value\n",
    "    \n",
    "        \n",
    "    def act(self, x, deterministic=False):\n",
    "        logit, _ = self.forward(x)\n",
    "        probs = F.softmax(logit, dim=-1)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = probs.max(1)[1]\n",
    "        else:\n",
    "            action = probs.multinomial(1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def evaluate_actions(self, x, action):\n",
    "        logit, value = self.forward(x)\n",
    "        \n",
    "        probs     = F.softmax(logit, dim=-1)\n",
    "        log_probs = F.log_softmax(logit, dim=-1)\n",
    "        \n",
    "        action_log_probs = log_probs.gather(1, action)\n",
    "        entropy = -(probs * log_probs).sum(1).mean()\n",
    "        \n",
    "        return logit, action_log_probs, value, entropy\n",
    "        \n",
    "    def feature_size(self):\n",
    "        return self.features(torch.zeros(1, *self.in_shape)).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_envs, state_shape):\n",
    "        self.num_steps = num_steps\n",
    "        self.num_envs = num_envs\n",
    "        self.states = torch.zeros(num_steps+1, num_envs, *state_shape)\n",
    "        self.rewards = torch.zeros(num_steps, num_envs, 1)\n",
    "        self.masks = torch.ones(num_steps+1, num_envs, 1)\n",
    "        self.actions = torch.zeros(num_steps, num_envs, 1).long()\n",
    "            \n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        self.states = self.states.to(device)\n",
    "        self.rewards = self.rewards.to(device)\n",
    "        self.masks = self.masks.to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        \n",
    "    def insert(self, step, state, action, reward, mask):\n",
    "        self.states[step+1].copy_(state)\n",
    "        self.actions[step].copy_(action)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.masks[step+1].copy_(mask)\n",
    "        \n",
    "    def after_update(self):\n",
    "        self.states[0].copy_(self.states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        \n",
    "    def compute_returns(self, next_value, gamma):\n",
    "        returns = torch.zeros(self.num_steps + 1, self.num_envs, 1).to(self.device)\n",
    "        returns[-1] = next_value\n",
    "        for step in reversed(range(self.num_steps)):\n",
    "            returns[step] = returns[step+1] * gamma * self.masks[step + 1] + self.rewards[step]\n",
    "        return returns[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"regular\"\n",
    "num_envs = 16\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = MiniPacman(mode, 1000)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "state_shape = envs.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a2c hyperparams:\n",
    "gamma = 0.99\n",
    "entropy_coef = 0.01\n",
    "value_loss_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "num_steps = 10\n",
    "num_frames = int(1e5)\n",
    "\n",
    "#rmsprop hyperparams:\n",
    "lr    = 7e-4\n",
    "eps   = 1e-5\n",
    "alpha = 0.99\n",
    "\n",
    "#Init a2c and rmsprop\n",
    "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
    "actor_critic.to(DEVICE)\n",
    "optimizer = optim.RMSprop(actor_critic.parameters(), lr, eps=eps, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = RolloutStorage(num_steps, num_envs, envs.observation_space.shape)\n",
    "rollout.to(DEVICE)\n",
    "\n",
    "all_rewards = []\n",
    "all_losses  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = envs.reset()\n",
    "state = torch.FloatTensor(np.float32(state)).to(DEVICE)\n",
    "\n",
    "    \n",
    "rollout.states[0].copy_(state)\n",
    "\n",
    "episode_rewards = torch.zeros(num_envs, 1)\n",
    "final_rewards   = torch.zeros(num_envs, 1)\n",
    "\n",
    "for i_update in range(num_frames):\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        action = actor_critic.act(state)\n",
    "        next_state, reward, done, _ = envs.step(action.squeeze(1).cpu().data.numpy())\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "        episode_rewards += reward\n",
    "        masks = torch.FloatTensor(1-np.array(done)).unsqueeze(1)\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1-masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "        \n",
    "        masks.to(DEVICE)\n",
    "\n",
    "        state = torch.FloatTensor(np.float32(next_state)).to(DEVICE)\n",
    "        rollout.insert(step, state, action.data, reward, masks)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, next_value = actor_critic(rollout.states[-1])\n",
    "        \n",
    "    next_value = next_value.data\n",
    "\n",
    "    returns = rollout.compute_returns(next_value, gamma)\n",
    "\n",
    "    logit, action_log_probs, values, entropy = actor_critic.evaluate_actions(\n",
    "        rollout.states[:-1].detach().view(-1, *state_shape),\n",
    "        rollout.actions.detach().view(-1, 1)\n",
    "    )\n",
    "\n",
    "    values = values.view(num_steps, num_envs, 1)\n",
    "    action_log_probs = action_log_probs.view(num_steps, num_envs, 1)\n",
    "    advantages = returns.detach() - values\n",
    "\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "    action_loss = -(advantages.detach() * action_log_probs).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(actor_critic.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i_update % 100 == 0:\n",
    "        all_rewards.append(final_rewards.mean())\n",
    "        all_losses.append(loss.data)\n",
    "        \n",
    "        display.clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title(f\"epoch {i_update}. reward: {np.mean(all_rewards[-10:])}\")\n",
    "        plt.plot(all_rewards)\n",
    "        plt.subplot(132)\n",
    "        plt.title(f\"loss {all_losses[-1].item()}\")\n",
    "        plt.plot(all_losses)\n",
    "        plt.show()\n",
    "        \n",
    "    rollout.after_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor_critic.state_dict(), \"actor_critic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading successfully!\n"
     ]
    }
   ],
   "source": [
    "mode = \"regular\"\n",
    "env = MiniPacman(mode, 1000)\n",
    "\n",
    "actor_critic = ActorCritic(env.observation_space.shape, env.action_space.n)\n",
    "checkpoint = torch.load(os.path.join(\"training\", \"actor_critic\", \"actor_critic_checkpoint\"), map_location=DEVICE)\n",
    "actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])\n",
    "actor_critic.to(DEVICE)\n",
    "\n",
    "print(\"loading successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = MiniPacman(mode, 1000)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "step = 1\n",
    "\n",
    "while not done:\n",
    "    current_state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "    action = actor_critic.act(current_state)\n",
    "    next_state, reward, done, _ = env.step(action.data[0, 0])\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    \n",
    "    plt.imshow(state.transpose([1, 2, 0]))\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"steps: {step}, reward: {total_reward}\")\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
