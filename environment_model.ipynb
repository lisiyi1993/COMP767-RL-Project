{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from utils.minipacman import MiniPacman\n",
    "from utils.multiprocessing_env import SubprocVecEnv\n",
    "from utils.actor_critic import ActorCritic, RolloutStorage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_CRITIC_PATH = os.path.join(\"training\", \"actor_critic\")\n",
    "ENVIRONMENT_PATH = os.path.join(\"training\", \"environment_model\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 different pixels in MiniPacman\n",
    "pixels = (\n",
    "    (0.0, 1.0, 1.0),\n",
    "    (0.0, 1.0, 0.0), \n",
    "    (0.0, 0.0, 1.0),\n",
    "    (1.0, 1.0, 1.0),\n",
    "    (1.0, 1.0, 0.0), \n",
    "    (0.0, 0.0, 0.0),\n",
    "    (1.0, 0.0, 0.0),\n",
    ")\n",
    "pixel_to_categorical = {pix:i for i, pix in enumerate(pixels)} \n",
    "num_pixels = len(pixels)\n",
    "\n",
    "#For each mode in MiniPacman there are different rewards\n",
    "mode_rewards = {\n",
    "    \"regular\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    \"avoid\":   [0.1, -0.1, -5, -10, -20],\n",
    "    \"hunt\":    [0, 1, 10, -20],\n",
    "    \"ambush\":  [0, -0.1, 10, -20],\n",
    "    \"rush\":    [0, -0.1, 9.9]\n",
    "}\n",
    "reward_to_categorical = {mode: {reward:i for i, reward in enumerate(mode_rewards[mode])} for mode in mode_rewards.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pix_to_target(next_states):\n",
    "    target = []\n",
    "    for pixel in next_states.transpose(0, 2, 3, 1).reshape(-1, 3):\n",
    "        target.append(pixel_to_categorical[tuple([np.ceil(pixel[0]), np.ceil(pixel[1]), np.ceil(pixel[2])])])\n",
    "    return target\n",
    "\n",
    "def target_to_pix(imagined_states):\n",
    "    pixels = []\n",
    "    to_pixel = {value: key for key, value in pixel_to_categorical.items()}\n",
    "    for target in imagined_states:\n",
    "        pixels.append(list(to_pixel[target]))\n",
    "    return np.array(pixels)\n",
    "\n",
    "def rewards_to_target(mode, rewards):\n",
    "    target = []\n",
    "    for reward in rewards:\n",
    "        target.append(reward_to_categorical[mode][reward])\n",
    "    return target\n",
    "\n",
    "def plot(frame_idx, rewards, losses):\n",
    "    display.clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('loss %s' % losses[-1])\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_shape, n1, n2, n3):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "        self.n3 = n3\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=in_shape[1:])\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0] * 2, n1, kernel_size=1, stride=2, padding=6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n1, n1, kernel_size=10, stride=1, padding=(5, 6)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0] * 2, n2, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n2, n2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(n1 + n2,  n3, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.pool_and_inject(inputs)\n",
    "        x = torch.cat([self.conv1(x), self.conv2(x)], 1)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat([x, inputs], 1)\n",
    "        return x\n",
    "    \n",
    "    def pool_and_inject(self, x):\n",
    "        pooled = self.maxpool(x)\n",
    "        tiled = pooled.expand((x.size(0),) + self.in_shape)\n",
    "        out = torch.cat([tiled, x], 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvModel(nn.Module):\n",
    "    def __init__(self, in_shape, num_actions, num_pixels, num_rewards):\n",
    "        super(EnvModel, self).__init__()\n",
    "        width  = in_shape[1]\n",
    "        height = in_shape[2]\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0] + num_actions, 64, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.basic_block1 = BasicBlock((64, width, height), 16, 32, 64)\n",
    "        self.basic_block2 = BasicBlock((128, width, height), 16, 32, 64)\n",
    "        \n",
    "        self.image_conv = nn.Sequential(\n",
    "            nn.Conv2d(192, 256, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.image_fc = nn.Linear(256, num_pixels)\n",
    "        \n",
    "        self.reward_conv = nn.Sequential(\n",
    "            nn.Conv2d(192, 64, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.reward_fc    = nn.Linear(64 * width * height, num_rewards)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.basic_block1(x)\n",
    "        x = self.basic_block2(x)\n",
    "        \n",
    "        image = self.image_conv(x)\n",
    "        image = image.permute(0, 2, 3, 1).contiguous().view(-1, 256)\n",
    "        image = self.image_fc(image)\n",
    "\n",
    "        reward = self.reward_conv(x)\n",
    "        reward = reward.view(batch_size, -1)\n",
    "        reward = self.reward_fc(reward)\n",
    "        \n",
    "        return image, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, actor_critic):\n",
    "    if state.ndim == 4:\n",
    "        state = torch.FloatTensor(np.float32(state))\n",
    "    else:\n",
    "        state = torch.FloatTensor(np.float32(state)).unsqueeze(0)\n",
    "    \n",
    "    state = state.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        action = actor_critic.act(state)\n",
    "        \n",
    "    action = action.data.cpu().squeeze(1).numpy()\n",
    "    return action\n",
    "\n",
    "def play_games(envs, frames, actor_critic):\n",
    "    states = envs.reset()\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        actions = get_action(states, actor_critic)\n",
    "        next_states, rewards, dones, _ = envs.step(actions)\n",
    "        \n",
    "        yield frame_idx, states, actions, rewards, next_states, dones\n",
    "        \n",
    "        states = next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"regular\"\n",
    "num_envs = 16\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = MiniPacman(mode, 1000)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "state_shape = envs.observation_space.shape\n",
    "num_actions = envs.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = EnvModel(envs.observation_space.shape, envs.action_space.n, num_pixels, len(mode_rewards[\"regular\"]))\n",
    "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(env_model.parameters())\n",
    "\n",
    "env_model = env_model.to(DEVICE)\n",
    "actor_critic = actor_critic.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(ACTOR_CRITIC_PATH, \"actor_critic_checkpoint\"))\n",
    "actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_coef = 0.1\n",
    "num_updates = 5000\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "\n",
    "for frame_idx, states, actions, rewards, next_states, dones in play_games(envs, num_updates, actor_critic):\n",
    "    states      = torch.FloatTensor(states)\n",
    "    actions     = torch.LongTensor(actions)\n",
    "\n",
    "    batch_size = states.size(0)\n",
    "    \n",
    "    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n",
    "    onehot_actions[range(batch_size), actions] = 1\n",
    "    inputs = torch.cat([states, onehot_actions], 1)\n",
    "    \n",
    "    inputs = inputs.to(DEVICE)\n",
    "\n",
    "    imagined_state, imagined_reward = env_model(inputs)\n",
    "\n",
    "    target_state = pix_to_target(next_states)\n",
    "    target_state = torch.LongTensor(target_state).to(DEVICE)\n",
    "    \n",
    "    target_reward = rewards_to_target(mode, rewards)\n",
    "    target_reward = torch.LongTensor(target_reward).to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    image_loss  = criterion(imagined_state, target_state)\n",
    "    reward_loss = criterion(imagined_reward, target_reward)\n",
    "    loss = image_loss + reward_coef * reward_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    all_rewards.append(np.mean(rewards))\n",
    "    \n",
    "    if frame_idx % 10 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading models successfully!\n"
     ]
    }
   ],
   "source": [
    "mode = \"regular\"\n",
    "env = MiniPacman(mode, 1000)\n",
    "batch_size = 1\n",
    "num_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "\n",
    "checkpoint = torch.load(os.path.join(\"training\", \"environment_model\", \"env_model\"), map_location=DEVICE)\n",
    "env_model = EnvModel(state_shape, num_actions, num_pixels, len(mode_rewards[\"regular\"]))\n",
    "env_model.load_state_dict(checkpoint)\n",
    "env_model.to(DEVICE)\n",
    "\n",
    "actor_critic = ActorCritic(state_shape, num_actions)\n",
    "checkpoint = torch.load(os.path.join(\"training\", \"actor_critic\", \"actor_critic_checkpoint\"), map_location=DEVICE)\n",
    "actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])\n",
    "actor_critic.to(DEVICE)\n",
    "\n",
    "print(\"loading models successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "done = False\n",
    "state = env.reset()\n",
    "iss = []\n",
    "ss  = []\n",
    "\n",
    "steps = 0\n",
    "total_imagined_rewards = 0\n",
    "total_reawards = 0\n",
    "\n",
    "while not done:\n",
    "    steps += 1\n",
    "    actions = get_action(state, actor_critic)\n",
    "    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n",
    "    onehot_actions[range(batch_size), actions] = 1\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    \n",
    "    inputs = torch.cat([state, onehot_actions], 1).to(DEVICE)\n",
    "\n",
    "    imagined_state, imagined_reward = env_model(inputs)\n",
    "    imagined_state = F.softmax(imagined_state, dim=-1)\n",
    "    iss.append(imagined_state)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(actions[0])\n",
    "    ss.append(state)\n",
    "    state = next_state\n",
    "    \n",
    "    imagined_image = target_to_pix(imagined_state.view(batch_size, -1, len(pixels))[0].max(1)[1].data.cpu().numpy())\n",
    "    imagined_image = imagined_image.reshape(15, 19, 3)\n",
    "    state_image = torch.FloatTensor(next_state).permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    total_imagined_rewards +=  F.softmax(imagined_reward, dim=-1).max(1)[1].item()\n",
    "    total_reawards += reward\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.title(f\"steps: {steps}, reward: {total_imagined_rewards} (Imagined)\")\n",
    "    plt.imshow(imagined_image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.title(f\"steps: {steps}, reward: {total_reawards} (Actual)\")\n",
    "    plt.imshow(state_image)\n",
    "    plt.axis('off')\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(os.path.join(\"results\", \"env_model_result.pdf\"), bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    if steps > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
