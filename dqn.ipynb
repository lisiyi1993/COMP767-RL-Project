{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from utils.minipacman import MiniPacman\n",
    "from utils.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.in_shape = in_shape\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),\n",
    "            # try adding batch norm\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Linear(256, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        value = self.head(x)\n",
    "        return value\n",
    "    \n",
    "\n",
    "    def act(self, x):\n",
    "        value = self.forward(x)\n",
    "        # probs = F.softmax(value, dim=-1)\n",
    "        return value.max(1)[1].view(-1, 1)\n",
    "        \n",
    "        # return probs.multinomial(1)\n",
    "    \n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.features(torch.zeros(1, *self.in_shape)).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net, num_actions, num_envs, epilson=0.9):\n",
    "    e = random.random()\n",
    "    \n",
    "    if e < epilson:\n",
    "        with torch.no_grad():\n",
    "            return policy_net.act(state)\n",
    "    else:\n",
    "        # return torch.tensor([[random.randrange(num_actions)]], device=DEVICE, dtype=torch.long)\n",
    "        return torch.from_numpy(np.random.randint(num_actions, size=(num_envs, 1))).long().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_net, target_net, memory, batch_size=128, gamma=0.999, memory_sample_prob=0.25):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), \n",
    "                                  device=DEVICE, dtype=torch.bool)\n",
    "    \n",
    "    non_final_next_states = torch.cat([torch.FloatTensor([s]).to(DEVICE) for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=DEVICE)\n",
    "    \n",
    "    # For DQN\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # For DDQN\n",
    "    # with torch.no_grad():\n",
    "    #         next_pred_action_batch = policy_net.act(non_final_next_states)\n",
    "        \n",
    "    # next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, next_pred_action_batch).squeeze().detach()\n",
    "    \n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # loss = F.mse_loss(state_action_values, expected_state_action_values.view(-1, 1))\n",
    "    # loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    target_net.eval()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"regular\"\n",
    "num_envs = 16\n",
    "# env = MiniPacman(mode, 1000)\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = MiniPacman(mode, 1000)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "state_shape = envs.observation_space.shape\n",
    "num_actions = envs.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(state_shape, num_actions).to(DEVICE) # save to checkpoint\n",
    "target_net = DQN(state_shape, num_actions).to(DEVICE) # save to checkpoint\n",
    "target_net.load_state_dict(policy_net.state_dict()) # save to checkpoint\n",
    "target_net.eval()\n",
    "policy_net.train()\n",
    "\n",
    "\n",
    "\n",
    "lr    = 7e-4\n",
    "eps   = 1e-5\n",
    "alpha = 0.99\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr, eps=eps, alpha=alpha) # save to checkpoint\n",
    "# optimizer = optim.Adam(policy_net.parameters())\n",
    "# optimizer = optim.RMSprop(policy_net.parameters()) # save to checkpoint\n",
    "memory = ReplayMemory(10000) # save to checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = int(1e5)\n",
    "target_update = 1000\n",
    "batch_size = 256\n",
    "backprops_freq = 0\n",
    "cur_best_reward = 0\n",
    "tau = 1e-3\n",
    "\n",
    "all_rewards = []\n",
    "# all_losses  = []\n",
    "\n",
    "episode_rewards = torch.zeros(num_envs, 1)\n",
    "final_rewards   = torch.zeros(num_envs, 1)\n",
    "\n",
    "state = envs.reset()\n",
    "state = torch.FloatTensor(np.float32(state)).to(DEVICE)\n",
    "\n",
    "for i_update in range(num_frames):\n",
    "    # Initialize the environment and state\n",
    "    state = torch.FloatTensor(np.float32(state)).to(DEVICE)\n",
    "    \n",
    "    for t in range(10):\n",
    "        # Select and perform an action\n",
    "        action = select_action(state, policy_net, num_actions, num_envs)\n",
    "        next_state, reward, done, _ = envs.step(action.squeeze(1).cpu().data.numpy())\n",
    "        reward = torch.tensor(reward).unsqueeze(1).to(DEVICE)\n",
    "        \n",
    "        episode_rewards += reward\n",
    "        masks = torch.FloatTensor(1-np.array(done)).unsqueeze(1)\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1-masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "        \n",
    "        # Store the all transition in memory\n",
    "        for i in range(num_envs):\n",
    "            memory.push(state[i].unsqueeze(0), action[i].unsqueeze(0), next_state[i], reward[i])\n",
    "    \n",
    "        # Move to the next state\n",
    "        state = torch.FloatTensor(np.float32(next_state)).to(DEVICE)\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model(policy_net, target_net, memory, batch_size=batch_size)\n",
    "             \n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        # hard update\n",
    "        backprops_freq += 1\n",
    "        if backprops_freq % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # soft update\n",
    "        # for target_param, local_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "        #     target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "    \n",
    "    \n",
    "    if i_update % 10 == 0:\n",
    "        all_rewards.append(final_rewards.mean())\n",
    "        display.clear_output(True)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(all_rewards)\n",
    "        plt.title(f\"epoch {i_update}. reward: {np.mean(all_rewards[-10:])}\")\n",
    "        plt.xlabel(\"Environmental Steps x10\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = MiniPacman(mode, 1000)\n",
    "# state = env.reset()\n",
    "# done = False\n",
    "# total_reward = 0\n",
    "# step = 1\n",
    "\n",
    "# policy_net.eval()\n",
    "\n",
    "# while not done:\n",
    "#     current_state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "# #     action = target_net.act(current_state)\n",
    "#     action = policy_net.act(current_state)\n",
    "#     next_state, reward, done, _ = env.step(action.data[0, 0])\n",
    "#     total_reward += reward\n",
    "#     state = next_state\n",
    "    \n",
    "#     plt.imshow(state.transpose([1, 2, 0]))\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"steps: {step}, reward: {total_reward}\")\n",
    "    \n",
    "#     display.display(plt.gcf())\n",
    "#     display.clear_output(wait=True)\n",
    "#     time.sleep(0.1)\n",
    "    \n",
    "#     step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
