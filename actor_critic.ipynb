{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from utils.minipacman import MiniPacman\n",
    "from utils.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):  \n",
    "    def __init__(self, in_shape, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.critic_network = nn.Linear(256, 1)\n",
    "        self.actor_network = nn.Linear(256, n_actions)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        logit = self.actor_network(x)\n",
    "        value = self.critic_network(x)\n",
    "        return logit, value\n",
    "    \n",
    "        \n",
    "    def act(self, x, deterministic=False):\n",
    "        logit, _ = self.forward(x)\n",
    "        probs = F.softmax(logit, dim=-1)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = probs.max(1)[1]\n",
    "        else:\n",
    "            action = probs.multinomial(1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def evaluate_actions(self, x, action):\n",
    "        logit, value = self.forward(x)\n",
    "        \n",
    "        probs     = F.softmax(logit, dim=-1)\n",
    "        log_probs = F.log_softmax(logit, dim=-1)\n",
    "        \n",
    "        action_log_probs = log_probs.gather(1, action)\n",
    "        entropy = -(probs * log_probs).sum(1).mean()\n",
    "        \n",
    "        return logit, action_log_probs, value, entropy\n",
    "        \n",
    "    def feature_size(self):\n",
    "        return self.features(torch.zeros(1, *self.in_shape)).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_envs, state_shape):\n",
    "        self.num_steps = num_steps\n",
    "        self.num_envs = num_envs\n",
    "        self.states = torch.zeros(num_steps+1, num_envs, *state_shape)\n",
    "        self.rewards = torch.zeros(num_steps, num_envs, 1)\n",
    "        self.masks = torch.ones(num_steps+1, num_envs, 1)\n",
    "        self.actions = torch.zeros(num_steps, num_envs, 1).long()\n",
    "            \n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        self.states = self.states.to(device)\n",
    "        self.rewards = self.rewards.to(device)\n",
    "        self.masks = self.masks.to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        \n",
    "    def insert(self, step, state, action, reward, mask):\n",
    "        self.states[step+1].copy_(state)\n",
    "        self.actions[step].copy_(action)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.masks[step+1].copy_(mask)\n",
    "        \n",
    "    def after_update(self):\n",
    "        self.states[0].copy_(self.states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        \n",
    "    def compute_returns(self, next_value, gamma):\n",
    "        returns = torch.zeros(self.num_steps + 1, self.num_envs, 1).to(self.device)\n",
    "        returns[-1] = next_value\n",
    "        for step in reversed(range(self.num_steps)):\n",
    "            returns[step] = returns[step+1] * gamma * self.masks[step + 1] + self.rewards[step]\n",
    "        return returns[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"regular\"\n",
    "num_envs = 16\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = MiniPacman(mode, 1000)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "state_shape = envs.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a2c hyperparams:\n",
    "gamma = 0.99\n",
    "entropy_coef = 0.01\n",
    "value_loss_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "num_steps = 10\n",
    "num_frames = int(1e5)\n",
    "\n",
    "#rmsprop hyperparams:\n",
    "lr    = 7e-4\n",
    "eps   = 1e-5\n",
    "alpha = 0.99\n",
    "\n",
    "#Init a2c and rmsprop\n",
    "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
    "actor_critic.to(DEVICE)\n",
    "optimizer = optim.RMSprop(actor_critic.parameters(), lr, eps=eps, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = RolloutStorage(num_steps, num_envs, envs.observation_space.shape)\n",
    "rollout.to(DEVICE)\n",
    "\n",
    "all_rewards = []\n",
    "all_losses  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = envs.reset()\n",
    "state = torch.FloatTensor(np.float32(state)).to(DEVICE)\n",
    "\n",
    "    \n",
    "rollout.states[0].copy_(state)\n",
    "\n",
    "episode_rewards = torch.zeros(num_envs, 1)\n",
    "final_rewards   = torch.zeros(num_envs, 1)\n",
    "\n",
    "for i_update in range(num_frames):\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        action = actor_critic.act(state)\n",
    "        next_state, reward, done, _ = envs.step(action.squeeze(1).cpu().data.numpy())\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "        episode_rewards += reward\n",
    "        masks = torch.FloatTensor(1-np.array(done)).unsqueeze(1)\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1-masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "        \n",
    "        masks.to(DEVICE)\n",
    "\n",
    "        state = torch.FloatTensor(np.float32(next_state)).to(DEVICE)\n",
    "        rollout.insert(step, state, action.data, reward, masks)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, next_value = actor_critic(rollout.states[-1])\n",
    "        \n",
    "    next_value = next_value.data\n",
    "\n",
    "    returns = rollout.compute_returns(next_value, gamma)\n",
    "\n",
    "    logit, action_log_probs, values, entropy = actor_critic.evaluate_actions(\n",
    "        rollout.states[:-1].detach().view(-1, *state_shape),\n",
    "        rollout.actions.detach().view(-1, 1)\n",
    "    )\n",
    "\n",
    "    values = values.view(num_steps, num_envs, 1)\n",
    "    action_log_probs = action_log_probs.view(num_steps, num_envs, 1)\n",
    "    advantages = returns.detach() - values\n",
    "\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "    action_loss = -(advantages.detach() * action_log_probs).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(actor_critic.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i_update % 100 == 0:\n",
    "        all_rewards.append(final_rewards.mean())\n",
    "        all_losses.append(loss.data)\n",
    "        \n",
    "        display.clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title(f\"epoch {i_update}. reward: {np.mean(all_rewards[-10:])}\")\n",
    "        plt.plot(all_rewards)\n",
    "        plt.subplot(132)\n",
    "        plt.title(f\"loss {all_losses[-1].item()}\")\n",
    "        plt.plot(all_losses)\n",
    "        plt.show()\n",
    "        \n",
    "    rollout.after_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor_critic.state_dict(), \"actor_critic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (critic_network): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (actor_network): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = \"regular\"\n",
    "num_envs = 16\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = MiniPacman(mode, 1000)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "state_shape = envs.observation_space.shape\n",
    "\n",
    "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
    "checkpoint = torch.load(os.path.join(\"training/actor_critic\", \"actor_critic_checkpoint\"), map_location=DEVICE)\n",
    "actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])\n",
    "actor_critic.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = MiniPacman(mode, 1000)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "step = 1\n",
    "\n",
    "while not done:\n",
    "    current_state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "    action = actor_critic.act(current_state)\n",
    "    next_state, reward, done, _ = env.step(action.data[0, 0])\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    \n",
    "    plt.imshow(state.transpose([1, 2, 0]))\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"steps: {step}, reward: {total_reward}\")\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
